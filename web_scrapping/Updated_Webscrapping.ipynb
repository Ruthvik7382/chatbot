{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635872ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib.parse import urljoin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbf2b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "os.chdir('/home/ec2-user/ITMT597/misc/files/urls_split')\n",
    "\n",
    "academic_programs = pd.read_csv('academic_programs.csv')\n",
    "student_service = pd.read_csv('student_service.csv')\n",
    "admissions_and_enrollment = pd.read_csv('admissions_and_enrollment.csv')\n",
    "admin_policy_info = pd.read_csv('admin_policy_info.csv')\n",
    "specialized_programs = pd.read_csv('specialized_programs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_commas_and_save(input_file):\n",
    "    try:\n",
    "        with open(input_file, 'r') as infile:\n",
    "            content = infile.read()\n",
    "            content_without_commas = content.replace(',', '')\n",
    "\n",
    "        with open(input_file, 'w') as outfile:\n",
    "            outfile.write(content_without_commas)\n",
    "            return input_file\n",
    "            \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60d6170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running through bulletin URL's\n",
    "def extract_data_and_save_b(url):\n",
    "\n",
    "    retry_strategy = Retry(\n",
    "    total=8,  # Number of maximum retries\n",
    "    backoff_factor=1,  # Exponential backoff factor\n",
    "    status_forcelist=[500, 502, 503, 504],  # HTTP status codes to retry on\n",
    "    )\n",
    "\n",
    "    # Create an HTTP session with retry settings\n",
    "    http = requests.Session()\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http.mount(\"http://\", adapter)\n",
    "    http.mount(\"https://\", adapter)\n",
    "\n",
    "    \n",
    "    try:\n",
    "    # Send a GET request using the HTTP session\n",
    "    #here 5 sec is connection_timeout where 27 sec is read time_out once the connection is established \n",
    "        response = requests.get(url, timeout=(5, 27))\n",
    "        \n",
    "        # Empty dictionary for storing headers and tables\n",
    "        data = {}\n",
    "        \n",
    "        sos_added = True\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            heading_text = None\n",
    "\n",
    "            # Find all relevant elements within <main>\n",
    "            for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'table','ul','div']):\n",
    "                tag_name = element.name\n",
    "\n",
    "                if tag_name == 'h1':\n",
    "                    heading_text1 = element.text.strip()\n",
    "                    if heading_text1:\n",
    "                        main_heading = heading_text1\n",
    "                 \n",
    "\n",
    "\n",
    "                if tag_name.startswith('h'):\n",
    "                    heading_text = element.text.strip()\n",
    "                    previous_heading = heading_text\n",
    "                    if heading_text:\n",
    "                        if sos_added:\n",
    "                            if tag_name == 'h1':\n",
    "                                heading_text = f'sos: {heading_text}'\n",
    "                            else:\n",
    "                                if main_heading is None:\n",
    "                                    main_heading = 'Academic Programs Details'\n",
    "                                heading_text = f'sos: {main_heading} <{heading_text}>'\n",
    "                        data[heading_text] = []\n",
    "                    \n",
    "\n",
    "                elif tag_name == 'p':\n",
    "                    if heading_text:\n",
    "                        passage_text = element.text.strip()\n",
    "                        data[heading_text].append(passage_text)\n",
    "                        \n",
    "\n",
    "                elif tag_name == 'ul':\n",
    "                    list_data = []\n",
    "                    for li in element.find_all('li'):\n",
    "                        bullet_point = li.text.strip()\n",
    "                        list_data.append(bullet_point)\n",
    "\n",
    "                    if heading_text:\n",
    "                        data[heading_text].extend(list_data)\n",
    "\n",
    "\n",
    "                elif tag_name == 'table':\n",
    "                    table_data = []\n",
    "                    a = False\n",
    "                    for row in element.find_all('tr'):\n",
    "                        row_data = [cell.text.strip() for cell in row.find_all(['th', 'td'])]\n",
    "                        if row_data == ['Year 1']:\n",
    "                            new_data = []\n",
    "                            new_data.append(row_data)  # Start a new list when 'Year 1' is encountered\n",
    "                            a = True\n",
    "                        elif a:\n",
    "                            new_data.append(row_data)  # Append subsequent rows to the new list\n",
    "                        else:\n",
    "                            table_data.append(row_data)\n",
    "                    if a:\n",
    "                        data1 = new_data\n",
    "                        last_item = data1[-1]\n",
    "                        columns = data1[1]\n",
    "                        df1 = pd.DataFrame(data1, columns=columns)\n",
    "\n",
    "                        year_word = None\n",
    "                        for index, row in df1.iterrows():\n",
    "                            if row[columns[0]].startswith('Year'):\n",
    "                                year_word = row[columns[0]]\n",
    "\n",
    "                            elif row[columns[0]].startswith('Semester'):\n",
    "                                df1.at[index, columns[0]] = f'{year_word}\\n{row[columns[0]]}'\n",
    "                                df1.at[index, columns[2]] = f'{year_word}\\n{row[columns[2]]}'\n",
    "\n",
    "                        df1 = df1.replace('None', pd.NA).dropna()\n",
    "                        new_df = df1.iloc[:, -2:]\n",
    "                        df1 = df1.iloc[:, :2]\n",
    "                        column_names = df1.columns\n",
    "                        new_df.columns = column_names\n",
    "                        result_df = pd.concat([df1, new_df], axis=0)\n",
    "                        result_list_of_lists = result_df.values.tolist()\n",
    "                        result_list_of_lists.append(last_item)\n",
    "                        for item in result_list_of_lists:\n",
    "                            table_data.append(item)\n",
    "                    if heading_text:\n",
    "                        if any(char.isalpha() for char in 'table_data[0][0]') and any(char.isdigit() for char in 'table_data[0][0]'):\n",
    "                            intro_text = f\"These are the {previous_heading} courses for the {main_heading}\"\n",
    "                            table_data.insert(0, [intro_text.replace(\"sos: \", \"\")])\n",
    "                            #table_data.pop(1)\n",
    "                            data[heading_text].append(table_data)\n",
    "                        else:\n",
    "                            intro_text = f\"These are the {table_data[0][0]} for the {main_heading} {heading_text} and the total credits are {table_data[0][1]}\"\n",
    "                            table_data.insert(0, [intro_text.replace(\"sos: \", \"\")])\n",
    "                            #table_data.pop(2)\n",
    "                            table_data.pop(1)\n",
    "                            data[heading_text].append(table_data)\n",
    "                            \n",
    "                elif tag_name == 'div' and 'courseblock' in element.get('class', []):\n",
    "                    course_code_elem = element.find(class_='coursecode')\n",
    "                    course_title_elem = element.find(class_='coursetitle')\n",
    "                    course_attrs_elem = element.find(class_='noindent courseblockattr hours')\n",
    "                    satisfies_elem = element.find(class_='noindent courseblockattr')\n",
    "\n",
    "                    # Check if elements are found before accessing their text attributes\n",
    "                    course_code = course_code_elem.text.strip() if course_code_elem else ''\n",
    "                    data[heading_text].append(course_code)\n",
    "                    course_title = course_title_elem.text.strip() if course_title_elem else ''\n",
    "                    data[heading_text].append(course_title)\n",
    "                    course_attrs = course_attrs_elem.get_text(\" \",strip=True) if course_attrs_elem else ''\n",
    "                    data[heading_text].append(course_attrs)\n",
    "                    satisfies = satisfies_elem.get_text(\" \",strip=True) if satisfies_elem else ''\n",
    "                    data[heading_text].append(satisfies)\n",
    "                    \n",
    "                elif tag_name == 'div' and 'cl-menu' in element.get('id', ''):\n",
    "                    break\n",
    "                        \n",
    "\n",
    "            output_file_path = f'{url_hash}.txt'\n",
    "                \n",
    "        else:\n",
    "            print(f\"Failed to retrieve URL: {url[0]}. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while fetching URL: {url[0]}. Exception: {e}\")\n",
    "\n",
    "\n",
    "  # Find all relevant elements\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for heading, content in data.items():\n",
    "            output_file.write(f\"{heading}\\n\")\n",
    "            for item in content:\n",
    "                if isinstance(item, str):\n",
    "                    output_file.write(f\"{item}\\n\")\n",
    "                elif isinstance(item, list):\n",
    "                    for row in item:\n",
    "                        output_file.write(f\"{', '.join(row)}\\n\")\n",
    "            output_file.write(\"\\n\")\n",
    "\n",
    "    final_output = remove_commas_and_save(output_file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' elif tag_name == \\'a\\':\\n    link_text = element.text.strip()\\n    link_href = element.get(\\'href\\')\\n    if link_href:\\n        # Check if the href is an absolute URL or a relative URL\\n        if link_href.startswith(\"http://\") or link_href.startswith(\"https://\"):\\n            cleaned_link = link_href\\n        else:\\n            # If it\\'s a relative URL, convert it to absolute by joining with the base URL\\n            cleaned_link = urljoin(url, link_href)\\n    else:\\n        cleaned_link = None  # Handle cases where href is missing\\n\\n    if heading_text and link_text and cleaned_link is not None:\\n        data[heading_text].append(f\\'Link: {link_text} {cleaned_link}\\')'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for running through IIT URL's\n",
    "def extract_data_and_save(url):\n",
    "\n",
    "    retry_strategy = Retry(\n",
    "    total=8,  # Number of maximum retries\n",
    "    backoff_factor=1,  # Exponential backoff factor\n",
    "    status_forcelist=[500, 502, 503, 504],  # HTTP status codes to retry on\n",
    "    )\n",
    "\n",
    "    # Create an HTTP session with retry settings\n",
    "    http = requests.Session()\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http.mount(\"http://\", adapter)\n",
    "    http.mount(\"https://\", adapter)\n",
    "\n",
    "    \n",
    "    try:\n",
    "    # Send a GET request using the HTTP session\n",
    "    #here 5 sec is connection_timeout where 27 sec is read time_out once the connection is established \n",
    "        response = requests.get(url, timeout=(5, 27))\n",
    "        \n",
    "        # Empty dictionary for storing headers and tables\n",
    "        data = {}\n",
    "        \n",
    "        sos_added = True\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            main = soup.find('main')\n",
    "\n",
    "            if main:\n",
    "                first_heading = None\n",
    "                heading_text = None\n",
    "\n",
    "                # Find all relevant elements within <main>\n",
    "                for element in main.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'table', 'ul','div']): #removed 'a'\n",
    "\n",
    "                    if element.find_parent('nav'):\n",
    "                        continue\n",
    "                    \n",
    "                    tag_name = element.name\n",
    "\n",
    "                    if tag_name.startswith('h'):\n",
    "                        heading_text = element.text.strip()\n",
    "                        if heading_text:\n",
    "                            if first_heading is None:\n",
    "                                first_heading = f'sos: {heading_text}'\n",
    "                                data[heading_text] = []\n",
    "                            else:\n",
    "                                data[heading_text] = []\n",
    "\n",
    "                    elif tag_name == 'p':\n",
    "                        if heading_text:\n",
    "                            passage_text = element.text.strip()\n",
    "                            if heading_text not in data:\n",
    "                                data[heading_text] = []\n",
    "                            data[heading_text].append(passage_text)\n",
    "\n",
    "                    elif tag_name == 'ul':\n",
    "                        list_data = []\n",
    "                        for li in element.find_all('li'):\n",
    "                            bullet_point = li.text.strip()\n",
    "                            list_data.append(bullet_point)\n",
    "\n",
    "                        if heading_text:\n",
    "                            data[heading_text].extend(list_data)\n",
    "\n",
    "                    elif tag_name == 'table':\n",
    "                        table_data = []\n",
    "                        for row in element.find_all('tr'):\n",
    "                            row_data = [cell.text.strip() for cell in row.find_all('td')]\n",
    "                            table_data.append(row_data)\n",
    "                        if heading_text:\n",
    "                            data[heading_text].append(table_data)\n",
    "\n",
    "                    elif tag_name == 'span' and 'profile-item__contact__item' in element.get('class', []):\n",
    "                        # Extract data from the location element\n",
    "                        info_type = element.find('i')\n",
    "                        if info_type:\n",
    "                            info_type = info_type['class'][1]\n",
    "                            info_text = element.get_text(strip=True)\n",
    "                            last_word = element['class'][-1]\n",
    "                            data[heading_text].append(f'{last_word}: {info_text}')\n",
    "\n",
    "                if first_heading:\n",
    "                    modified_data = {}\n",
    "                    for key in data:\n",
    "                        if key != first_heading:\n",
    "                            modified_key = f'{first_heading} <{key}>'\n",
    "                            modified_data[modified_key] = data[key]\n",
    "                        else:\n",
    "                            modified_data[first_heading] = data[key]\n",
    "\n",
    "                #    return first_heading, modified_data\n",
    "                \n",
    "                #combined_data = '\\n'.join(map(str, data))\n",
    "\n",
    "                output_file_path = f'{url_hash}.txt'\n",
    "                \n",
    "        else:\n",
    "            print(f\"Failed to retrieve URL: {url[0]}. Status code: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while fetching URL: {url[0]}. Exception: {e}\")\n",
    "\n",
    "\n",
    "  # Find all relevant elements\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for heading, content in modified_data.items():\n",
    "            output_file.write(f\"{heading}\\n\")\n",
    "            for item in content:\n",
    "                if isinstance(item, str):\n",
    "                    output_file.write(f\"{item}\\n\")\n",
    "                elif isinstance(item, list):\n",
    "                    for row in item:\n",
    "                        output_file.write(f\"{', '.join(row)}\\n\")\n",
    "            output_file.write(\"\\n\")\n",
    "                    \n",
    "\n",
    "\"\"\" elif tag_name == 'a':\n",
    "    link_text = element.text.strip()\n",
    "    link_href = element.get('href')\n",
    "    if link_href:\n",
    "        # Check if the href is an absolute URL or a relative URL\n",
    "        if link_href.startswith(\"http://\") or link_href.startswith(\"https://\"):\n",
    "            cleaned_link = link_href\n",
    "        else:\n",
    "            # If it's a relative URL, convert it to absolute by joining with the base URL\n",
    "            cleaned_link = urljoin(url, link_href)\n",
    "    else:\n",
    "        cleaned_link = None  # Handle cases where href is missing\n",
    "\n",
    "    if heading_text and link_text and cleaned_link is not None:\n",
    "        data[heading_text].append(f'Link: {link_text} {cleaned_link}')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d7cab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/ITMT597/misc/files/urls_split/academic_programs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "456it [01:26,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "##webscrapping academic programs urls\n",
    "\n",
    "#creating df object that would contain all the urls \n",
    "df = academic_programs\n",
    "\n",
    "#list for url & hash mapping\n",
    "list_urls=[]\n",
    "\n",
    "os.chdir('/home/ec2-user/ITMT597/misc/files/urls_split/academic_programs')\n",
    "print(os.getcwd())\n",
    "\n",
    "#iterating over each of the URLs for webscrapping\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    url = row['urls']\n",
    "    url_hash = hash(url)\n",
    "\n",
    "    # Extract data from the current URL and save it to a text file\n",
    "    extract_data_and_save_b(url)\n",
    "\n",
    "    #URL and URL_hash mapping\n",
    "    list_urls.append([url, url_hash])\n",
    "    \n",
    "\n",
    "    # Add the text file to the zip archive\n",
    "    #zip_file.write(f'extracted_data_{url_hash}.txt', os.path.join(output_dir, f'extracted_data_{url_hash}.txt'))\n",
    "\n",
    "df_url = pd.DataFrame(list_urls, columns=['url', 'hash'])\n",
    "df_url.to_csv('url_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5192b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text saved to /home/ec2-user/ITMT597/misc/files/urls_split/academic_programs/combined_academic_programs.txt\n"
     ]
    }
   ],
   "source": [
    "##now combining all the text files into one file\n",
    "directory_path = '/home/ec2-user/ITMT597/misc/files/urls_split/academic_programs'\n",
    "# Specify the name of the output combined file\n",
    "output_file_name = 'combined_academic_programs.txt'\n",
    "\n",
    "# Create a list to store the content of each file\n",
    "file_contents = []\n",
    "\n",
    "# Loop through all .txt files in the directory\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and read the content of each .txt file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_contents.append(file.read())\n",
    "\n",
    "# Combine the contents of all files into one string\n",
    "combined_text = '\\n'.join(file_contents)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "output_file_path = os.path.join(directory_path, output_file_name)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"Combined text saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = student_service\n",
    "\n",
    "#list for url & hash mapping\n",
    "list_urls=[]\n",
    "\n",
    "os.chdir('/home/ec2-user/ITMT597/misc/files/urls_split/student_service')\n",
    "print(os.getcwd())\n",
    "\n",
    "#iterating over each of the URLs for webscrapping\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    url = row['urls']\n",
    "    url_hash = hash(url)\n",
    "\n",
    "    # Extract data from the current URL and save it to a text file\n",
    "    extract_data_and_save(url)\n",
    "\n",
    "    #URL and URL_hash mapping\n",
    "    list_urls.append([url, url_hash])\n",
    "    \n",
    "\n",
    "    # Add the text file to the zip archive\n",
    "    #zip_file.write(f'extracted_data_{url_hash}.txt', os.path.join(output_dir, f'extracted_data_{url_hash}.txt'))\n",
    "\n",
    "df_url = pd.DataFrame(list_urls, columns=['url', 'hash'])\n",
    "df_url.to_csv('url_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now combining all the text files into one file\n",
    "directory_path = '/home/ec2-user/ITMT597/misc/files/urls_split/student_service'\n",
    "# Specify the name of the output combined file\n",
    "output_file_name = 'combined_student_service.txt'\n",
    "\n",
    "# Create a list to store the content of each file\n",
    "file_contents = []\n",
    "\n",
    "# Loop through all .txt files in the directory\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and read the content of each .txt file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_contents.append(file.read())\n",
    "\n",
    "# Combine the contents of all files into one string\n",
    "combined_text = '\\n'.join(file_contents)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "output_file_path = os.path.join(directory_path, output_file_name)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"Combined text saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = admissions_and_enrollment\n",
    "\n",
    "#list for url & hash mapping\n",
    "list_urls=[]\n",
    "\n",
    "os.chdir('/home/ec2-user/ITMT597/misc/files/urls_split/admissions_and_enrollment')\n",
    "\n",
    "\n",
    "#iterating over each of the URLs for webscrapping\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    url = row['urls']\n",
    "    url_hash = hash(url)\n",
    "\n",
    "    # Extract data from the current URL and save it to a text file\n",
    "    extract_data_and_save(url)\n",
    "\n",
    "    #URL and URL_hash mapping\n",
    "    list_urls.append([url, url_hash])\n",
    "    \n",
    "\n",
    "    # Add the text file to the zip archive\n",
    "    #zip_file.write(f'extracted_data_{url_hash}.txt', os.path.join(output_dir, f'extracted_data_{url_hash}.txt'))\n",
    "\n",
    "df_url = pd.DataFrame(list_urls, columns=['url', 'hash'])\n",
    "df_url.to_csv('url_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now combining all the text files into one file\n",
    "directory_path = '/home/ec2-user/ITMT597/misc/files/urls_split/admissions_and_enrollment'\n",
    "# Specify the name of the output combined file\n",
    "output_file_name = 'combined_admissions_and_enrollment.txt'\n",
    "\n",
    "# Create a list to store the content of each file\n",
    "file_contents = []\n",
    "\n",
    "# Loop through all .txt files in the directory\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and read the content of each .txt file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_contents.append(file.read())\n",
    "\n",
    "# Combine the contents of all files into one string\n",
    "combined_text = '\\n'.join(file_contents)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "output_file_path = os.path.join(directory_path, output_file_name)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"Combined text saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/ITMT597/misc/files/urls_split/admin_policy_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:01, 23.37it/s]\n",
      "63it [00:09,  6.35it/s]\n"
     ]
    }
   ],
   "source": [
    "df = admin_policy_info.iloc[0:43,:]\n",
    "df2 = admin_policy_info.iloc[43:-1,:]\n",
    "\n",
    "#list for url & hash mapping\n",
    "list_urls=[]\n",
    "\n",
    "os.chdir('/home/ec2-user/ITMT597/misc/files/urls_split/admin_policy_info')\n",
    "print(os.getcwd())\n",
    "\n",
    "#iterating over each of the URLs for webscrapping\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    url = row['urls']\n",
    "    url_hash = hash(url)\n",
    "\n",
    "    # Extract data from the current URL and save it to a text file\n",
    "    extract_data_and_save(url)\n",
    "\n",
    "    #URL and URL_hash mapping\n",
    "    list_urls.append([url, url_hash])\n",
    "    \n",
    "\n",
    "    # Add the text file to the zip archive\n",
    "    #zip_file.write(f'extracted_data_{url_hash}.txt', os.path.join(output_dir, f'extracted_data_{url_hash}.txt'))\n",
    "\n",
    "df_url = pd.DataFrame(list_urls, columns=['url', 'hash'])\n",
    "df_url.to_csv('url_mapping.csv')\n",
    "\n",
    "list_urls2=[]\n",
    "for index, row in tqdm(df2.iterrows()):\n",
    "    url = row['urls']\n",
    "    url_hash = hash(url)\n",
    "\n",
    "    # Extract data from the current URL and save it to a text file\n",
    "    extract_data_and_save_b(url)\n",
    "\n",
    "    #URL and URL_hash mapping\n",
    "    list_urls2.append([url, url_hash])\n",
    "    \n",
    "\n",
    "    # Add the text file to the zip archive\n",
    "    #zip_file.write(f'extracted_data_{url_hash}.txt', os.path.join(output_dir, f'extracted_data_{url_hash}.txt'))\n",
    "\n",
    "df_url2 = pd.DataFrame(list_urls2, columns=['url', 'hash'])\n",
    "df_url2.to_csv('url_mapping2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text saved to /home/ec2-user/ITMT597/misc/files/urls_split/admin_policy_info/combined_admin_policy_info.txt\n"
     ]
    }
   ],
   "source": [
    "##now combining all the text files into one file\n",
    "directory_path = '/home/ec2-user/ITMT597/misc/files/urls_split/admin_policy_info'\n",
    "# Specify the name of the output combined file\n",
    "output_file_name = 'combined_admin_policy_info.txt'\n",
    "\n",
    "# Create a list to store the content of each file\n",
    "file_contents = []\n",
    "\n",
    "# Loop through all .txt files in the directory\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and read the content of each .txt file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_contents.append(file.read())\n",
    "\n",
    "# Combine the contents of all files into one string\n",
    "combined_text = '\\n'.join(file_contents)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "output_file_path = os.path.join(directory_path, output_file_name)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"Combined text saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = specialized_programs\n",
    "\n",
    "#list for url & hash mapping\n",
    "list_urls=[]\n",
    "\n",
    "os.chdir('/home/ec2-user/ITMT597/misc/files/urls_split/specialized_programs')\n",
    "\n",
    "\n",
    "#iterating over each of the URLs for webscrapping\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    url = row['urls']\n",
    "    url_hash = hash(url)\n",
    "\n",
    "    # Extract data from the current URL and save it to a text file\n",
    "    extract_data_and_save(url)\n",
    "\n",
    "    #URL and URL_hash mapping\n",
    "    list_urls.append([url, url_hash])\n",
    "    \n",
    "\n",
    "    # Add the text file to the zip archive\n",
    "    #zip_file.write(f'extracted_data_{url_hash}.txt', os.path.join(output_dir, f'extracted_data_{url_hash}.txt'))\n",
    "\n",
    "df_url = pd.DataFrame(list_urls, columns=['url', 'hash'])\n",
    "df_url.to_csv('url_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now combining all the text files into one file\n",
    "directory_path = '/home/ec2-user/ITMT597/misc/files/urls_split/specialized_programs'\n",
    "# Specify the name of the output combined file\n",
    "output_file_name = 'combined_specialized_programs.txt'\n",
    "\n",
    "# Create a list to store the content of each file\n",
    "file_contents = []\n",
    "\n",
    "# Loop through all .txt files in the directory\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and read the content of each .txt file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_contents.append(file.read())\n",
    "\n",
    "# Combine the contents of all files into one string\n",
    "combined_text = '\\n'.join(file_contents)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "output_file_path = os.path.join(directory_path, output_file_name)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"Combined text saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path1 ='/home/ec2-user/ITMT597/misc/files/urls_split/admin_policy_info/combined_admin_policy_info.txt'  \n",
    "file_path2 ='/home/ec2-user/ITMT597/misc/files/urls_split/admissions_and_enrollment/combined_admissions_and_enrollment.txt' \n",
    "file_path3 ='/home/ec2-user/ITMT597/misc/files/urls_split/student_service/combined_student_service.txt'   \n",
    "file_path4 ='/home/ec2-user/ITMT597/misc/files/urls_split/specialized_programs/combined_specialized_programs.txt' \n",
    "\n",
    "# Text to be removed\n",
    "text_to_remove = \"sos: Print Options\\nPrint this page.\\nThe PDF will include all information unique to this page.\"\n",
    "\n",
    "paths = [file_path1, file_path2, file_path3, file_path4]\n",
    "\n",
    "for file_path in paths:\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    # Replace the unwanted text with an empty string\n",
    "    modified_content = file_content.replace(text_to_remove, '')\n",
    "\n",
    "    # Write the modified content back to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(modified_content)\n",
    "\n",
    "    print(\"Text removed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text saved to /home/ec2-user/ITMT597/misc/files/urls_split/no_split_data/combined_all.txt\n"
     ]
    }
   ],
   "source": [
    "##creating no split text file\n",
    "directory_path = '/home/ec2-user/ITMT597/misc/files/urls_split/no_split_data'\n",
    "# Specify the name of the output combined file\n",
    "output_file_name = 'combined_all.txt'\n",
    "\n",
    "# Create a list to store the content of each file\n",
    "file_contents = []\n",
    "\n",
    "# Loop through all .txt files in the directory\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open and read the content of each .txt file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_contents.append(file.read())\n",
    "\n",
    "# Combine the contents of all files into one string\n",
    "combined_text = '\\n'.join(file_contents)\n",
    "\n",
    "# Write the combined text to the output file\n",
    "output_file_path = os.path.join(directory_path, output_file_name)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"Combined text saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
